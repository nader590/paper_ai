import streamlit as st
from transformers import MT5ForConditionalGeneration, MT5Tokenizer, pipeline
import PyPDF2

# ================= ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ =================
@st.cache_resource
def load_model():
    MODEL_NAME = "google/mt5-small"
    tokenizer = MT5Tokenizer.from_pretrained(MODEL_NAME)
    model = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME)
    generator = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
    return generator

generator = load_model()

# ================= Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© =================
def extract_text_from_pdf(file) -> str:
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        if page.extract_text():
            text += page.extract_text() + "\n"
    return text.strip()

def ask_model(prompt: str, max_new_tokens=500):
    outputs = generator(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7
    )
    return outputs[0]["generated_text"]

# ================= Streamlit App =================
st.title("ğŸ“„ AI Paper Assistant")

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù„ØºØ©
lang = st.radio("ğŸŒ Ø§Ø®ØªØ± Ø§Ù„Ù„ØºØ© / Choose Language", ["Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©", "English"])

# Labels Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©
labels = {
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": {
        "upload": "Ø§Ø±ÙØ¹ ÙˆØ±Ù‚Ø© PDF",
        "uploaded": "âœ… ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­",
        "extracted": "ğŸ“‘ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„ÙˆØ±Ù‚Ø©",
        "analyze": "ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ±Ù‚Ø©",
        "classify": "ğŸ“Œ Ø§Ù„ØªØµÙ†ÙŠÙ",
        "summary": "ğŸ“ Ø§Ù„Ù…Ù„Ø®Øµ",
        "ask": "â“ Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„ÙˆØ±Ù‚Ø©",
        "q_input": "Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§",
        "answer_btn": "Ø¥Ø¬Ø§Ø¨Ø©",
        "answer": "ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©",
    },
    "English": {
        "upload": "Upload PDF paper",
        "uploaded": "âœ… File uploaded successfully",
        "extracted": "ğŸ“‘ Extracted text",
        "analyze": "ğŸ” Analyze Paper",
        "classify": "ğŸ“Œ Classification",
        "summary": "ğŸ“ Summary",
        "ask": "â“ Ask a question about the paper",
        "q_input": "Type your question here",
        "answer_btn": "Answer",
        "answer": "ğŸ’¡ Answer",
    }
}

uploaded_file = st.file_uploader(labels[lang]["upload"], type=["pdf"])

if uploaded_file:
    st.success(labels[lang]["uploaded"])

    text = extract_text_from_pdf(uploaded_file)
    st.subheader(labels[lang]["extracted"])
    st.text_area("Extracted Text", text[:3000] + ("..." if len(text) > 3000 else ""), height=200)

    if st.button(labels[lang]["analyze"]):
        with st.spinner("â³ Processing..."):

            if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                classify_prompt = f"ØµÙ†Ù‘Ù Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø­Ø³Ø¨ Ù…Ø¬Ø§Ù„Ù‡Ø§ ÙˆÙ†ÙˆØ¹Ù‡Ø§:\n{text[:1500]}"
                summary_prompt = f"Ù„Ø®Ù‘Øµ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ±Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø©:\n{text[:2000]}"
            else:
                classify_prompt = f"Classify this research paper by its field and type:\n{text[:1500]}"
                summary_prompt = f"Summarize this research paper in clear points in English:\n{text[:2000]}"

            classification = ask_model(classify_prompt, max_new_tokens=200)
            summary = ask_model(summary_prompt, max_new_tokens=400)

        st.subheader(labels[lang]["classify"])
        st.write(classification)

        st.subheader(labels[lang]["summary"])
        st.write(summary)

    st.subheader(labels[lang]["ask"])
    question = st.text_input(labels[lang]["q_input"])
    if st.button(labels[lang]["answer_btn"]):
        if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
            qa_prompt = f"Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† ÙˆØ±Ù‚Ø© Ø¹Ù„Ù…ÙŠØ©:\n{text[:1500]}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
        else:
            qa_prompt = f"The following text is from a research paper:\n{text[:1500]}\n\nQuestion: {question}\nAnswer:"
        
        answer = ask_model(qa_prompt, max_new_tokens=300)
        st.subheader(labels[lang]["answer"])
        st.write(answer)
